{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PySpark Dubber","text":"<p>A compatibility layer for the <code>pyspark</code> API, allowing you to run <code>pyspark</code> code on backends such as DuckDB and Polars without porting your code.</p>"},{"location":"#why","title":"Why","text":"<p>Lately, SQL engines and DataFrame libraries such as DuckDB and Polars have become popular, offering great performance for non-distributed analytical workflows up to relatively large datasets (tens of GBs). For these sizes and below, Spark adds a lot of overhead and its startup time is relatively slow, making it not very cost- and time-efficient.</p> <p>However, Spark is still the most mature and widely used data processing framework, meaning that many people and organizations have large codebases relying on its APIs.</p> <p><code>pyspark-dubber</code> is a library that allows you to run <code>pyspark</code> code on many backends, such as DuckDB and Polars (actually any backend supported by ibis at this time), making it possible to migrate old code to a new backend with minimal changes.</p> <p>The aspiration of <code>pyspark-dubber</code> is be bug-for-bug compatible with <code>pyspark</code>.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>You can find API documentation and more information about the project in our documentation page.</p>"},{"location":"api_coverage/","title":"API Coverage","text":"<p>This page shows which APIs are currently re-implemented by <code>pyspark-dubber</code>. This list is not exhaustive, showing mostly public functions and DataFrame APIs, however some additional APIs and magic methods are also implemented.</p> <p>In addition to that, certain pyspark APIs are partially implemented, for example not all parameters or parameter types are supported. In spite of that, they are listed as implemented in the tables below, with notes in case of partial implementation.</p> <p>The overall approximate API coverage (with the caveats above) is 29.2%. We prioritize implementing commonly used functions, as pyspark has many esoteric APIs.</p>"},{"location":"api_coverage/#sparksession-319-16","title":"SparkSession (3/19 = 16%)","text":"API Implemented Notes <code>SparkSession.Builder</code> <code>SparkSession.active</code> <code>SparkSession.addArtifact</code> <code>SparkSession.addArtifacts</code> <code>SparkSession.addTag</code> <code>SparkSession.clearTags</code> <code>SparkSession.copyFromLocalToFs</code> <code>SparkSession.createDataFrame</code> <code>SparkSession.getActiveSession</code> <code>SparkSession.getTags</code> <code>SparkSession.interruptAll</code> <code>SparkSession.interruptOperation</code> <code>SparkSession.interruptTag</code> <code>SparkSession.newSession</code> <code>SparkSession.range</code> <code>SparkSession.removeTag</code> <code>SparkSession.sql</code> <code>SparkSession.stop</code> <code>SparkSession.table</code>"},{"location":"api_coverage/#sparksessionbuilder-37-43","title":"SparkSession.builder (3/7 = 43%)","text":"API Implemented Notes <code>Builder.appName</code> <code>Builder.config</code> <code>Builder.create</code> <code>Builder.enableHiveSupport</code> <code>Builder.getOrCreate</code> <code>Builder.master</code> <code>Builder.remote</code>"},{"location":"api_coverage/#input-formats-212-17","title":"Input Formats (2/12 = 17%)","text":"API Implemented Notes <code>DataFrameReader.csv</code> <code>DataFrameReader.format</code> <code>DataFrameReader.jdbc</code> <code>DataFrameReader.json</code> Most parameters are accepted but completely ignored. <code>DataFrameReader.load</code> <code>DataFrameReader.option</code> <code>DataFrameReader.options</code> <code>DataFrameReader.orc</code> <code>DataFrameReader.parquet</code> <code>DataFrameReader.schema</code> <code>DataFrameReader.table</code> <code>DataFrameReader.text</code>"},{"location":"api_coverage/#output-formats-316-19","title":"Output Formats (3/16 = 19%)","text":"API Implemented Notes <code>DataFrameWriter.bucketBy</code> <code>DataFrameWriter.csv</code> <code>DataFrameWriter.format</code> <code>DataFrameWriter.insertInto</code> <code>DataFrameWriter.jdbc</code> <code>DataFrameWriter.json</code> <code>DataFrameWriter.mode</code> <code>DataFrameWriter.option</code> <code>DataFrameWriter.options</code> <code>DataFrameWriter.orc</code> <code>DataFrameWriter.parquet</code> <code>DataFrameWriter.partitionBy</code> <code>DataFrameWriter.save</code> <code>DataFrameWriter.saveAsTable</code> <code>DataFrameWriter.sortBy</code> <code>DataFrameWriter.text</code>"},{"location":"api_coverage/#dataframe-3494-36","title":"DataFrame (34/94 = 36%)","text":"API Implemented Notes <code>DataFrame.agg</code> <code>DataFrame.alias</code> <code>DataFrame.approxQuantile</code> <code>DataFrame.cache</code> <code>DataFrame.checkpoint</code> <code>DataFrame.coalesce</code> <code>DataFrame.colRegex</code> <code>DataFrame.collect</code> <code>DataFrame.corr</code> <code>DataFrame.count</code> <code>DataFrame.cov</code> <code>DataFrame.createGlobalTempView</code> <code>DataFrame.createOrReplaceGlobalTempView</code> <code>DataFrame.createOrReplaceTempView</code> <code>DataFrame.createTempView</code> <code>DataFrame.crossJoin</code> pyspark allows duplicate column names, and by default does not prefix/suffix the columns of the other dataframe at all. Our backend (ibis) currently does not support duplicate column names, so this function suffixes all columns on other with '_right'. <code>DataFrame.crosstab</code> <code>DataFrame.cube</code> <code>DataFrame.describe</code> <code>DataFrame.distinct</code> <code>DataFrame.drop</code> Our backend (ibis) does not support duplicate column names like pyspark, therefore this function does not support dropping columns with the same name. You cannot anyway currently create such dataframes using <code>pyspark-dubber</code>. <code>DataFrame.dropDuplicates</code> <code>DataFrame.dropDuplicatesWithinWatermark</code> <code>DataFrame.drop_duplicates</code> <code>DataFrame.dropna</code> <code>DataFrame.exceptAll</code> <code>DataFrame.explain</code> <code>DataFrame.fillna</code> <code>DataFrame.filter</code> Using a string as a SQL expressions is not supported yet. <code>DataFrame.first</code> <code>DataFrame.foreach</code> <code>DataFrame.foreachPartition</code> <code>DataFrame.freqItems</code> <code>DataFrame.groupBy</code> Currently only column names are supported for grouping, column expressions are not supported. <code>DataFrame.groupby</code> Currently only column names are supported for grouping, column expressions are not supported. <code>DataFrame.head</code> <code>DataFrame.hint</code> <code>DataFrame.inputFiles</code> <code>DataFrame.intersect</code> <code>DataFrame.intersectAll</code> <code>DataFrame.isEmpty</code> <code>DataFrame.isLocal</code> <code>DataFrame.join</code> <code>DataFrame.limit</code> <code>DataFrame.localCheckpoint</code> <code>DataFrame.mapInArrow</code> <code>DataFrame.mapInPandas</code> <code>DataFrame.melt</code> <code>DataFrame.observe</code> <code>DataFrame.offset</code> <code>DataFrame.orderBy</code> Sorting by column ordinals (which are 1-based, not 0-based) is not supported yet. Additionally, this function still needs better testing around edge cases, when sorting with complex column expressions which include sorting. <code>DataFrame.pandas_api</code> <code>DataFrame.persist</code> <code>DataFrame.printSchema</code> <code>DataFrame.randomSplit</code> <code>DataFrame.registerTempTable</code> <code>DataFrame.repartition</code> <code>DataFrame.repartitionByRange</code> <code>DataFrame.replace</code> <code>DataFrame.rollup</code> <code>DataFrame.sameSemantics</code> <code>DataFrame.sample</code> <code>DataFrame.sampleBy</code> <code>DataFrame.select</code> <code>DataFrame.selectExpr</code> <code>DataFrame.semanticHash</code> <code>DataFrame.show</code> The <code>truncate</code> and <code>vertical</code> parameters are not honored. Additionally, the output is not printed justified exactly as pyspark as of the current version. <code>DataFrame.sort</code> Sorting by column ordinals (which are 1-based, not 0-based) is not supported yet. Additionally, this function still needs better testing around edge cases, when sorting with complex column expressions which include sorting. <code>DataFrame.sortWithinPartitions</code> <code>DataFrame.subtract</code> <code>DataFrame.summary</code> <code>DataFrame.tail</code> <code>DataFrame.take</code> <code>DataFrame.to</code> <code>DataFrame.toDF</code> <code>DataFrame.toJSON</code> <code>DataFrame.toLocalIterator</code> <code>DataFrame.toPandas</code> <code>DataFrame.to_koalas</code> <code>DataFrame.to_pandas_on_spark</code> <code>DataFrame.transform</code> <code>DataFrame.union</code> <code>DataFrame.unionAll</code> <code>DataFrame.unionByName</code> <code>DataFrame.unpersist</code> <code>DataFrame.unpivot</code> <code>DataFrame.where</code> Using a string as a SQL expressions is not supported yet. <code>DataFrame.withColumn</code> <code>DataFrame.withColumnRenamed</code> <code>DataFrame.withColumns</code> <code>DataFrame.withColumnsRenamed</code> <code>DataFrame.withMetadata</code> <code>DataFrame.withWatermark</code> <code>DataFrame.writeTo</code>"},{"location":"api_coverage/#groupby-212-17","title":"GroupBy (2/12 = 17%)","text":"API Implemented Notes <code>GroupedData.agg</code> <code>GroupedData.apply</code> <code>GroupedData.applyInPandas</code> <code>GroupedData.applyInPandasWithState</code> <code>GroupedData.avg</code> <code>GroupedData.cogroup</code> <code>GroupedData.count</code> <code>GroupedData.max</code> <code>GroupedData.mean</code> <code>GroupedData.min</code> <code>GroupedData.pivot</code> <code>GroupedData.sum</code>"},{"location":"api_coverage/#column-2532-78","title":"Column (25/32 = 78%)","text":"API Implemented Notes <code>Column.alias</code> <code>Column.asc</code> <code>Column.asc_nulls_first</code> <code>Column.asc_nulls_last</code> <code>Column.astype</code> <code>Column.between</code> <code>Column.bitwiseAND</code> <code>Column.bitwiseOR</code> <code>Column.bitwiseXOR</code> <code>Column.cast</code> <code>Column.contains</code> <code>Column.desc</code> <code>Column.desc_nulls_first</code> <code>Column.desc_nulls_last</code> <code>Column.dropFields</code> <code>Column.endswith</code> <code>Column.eqNullSafe</code> <code>Column.getField</code> <code>Column.getItem</code> <code>Column.ilike</code> <code>Column.isNotNull</code> <code>Column.isNull</code> <code>Column.isin</code> <code>Column.like</code> <code>Column.name</code> <code>Column.otherwise</code> <code>Column.over</code> <code>Column.rlike</code> <code>Column.startswith</code> <code>Column.substr</code> <code>Column.when</code> <code>Column.withField</code>"},{"location":"api_coverage/#functions-93422-22","title":"Functions (93/422 = 22%)","text":"API Implemented Notes <code>pyspark.sql.functions.abs</code> <code>pyspark.sql.functions.acos</code> <code>pyspark.sql.functions.acosh</code> <code>pyspark.sql.functions.add_months</code> <code>pyspark.sql.functions.aes_decrypt</code> <code>pyspark.sql.functions.aes_encrypt</code> <code>pyspark.sql.functions.aggregate</code> <code>pyspark.sql.functions.any_value</code> <code>pyspark.sql.functions.approxCountDistinct</code> <code>pyspark.sql.functions.approx_count_distinct</code> <code>pyspark.sql.functions.approx_percentile</code> <code>pyspark.sql.functions.array</code> <code>pyspark.sql.functions.array_agg</code> <code>pyspark.sql.functions.array_append</code> <code>pyspark.sql.functions.array_compact</code> <code>pyspark.sql.functions.array_contains</code> <code>pyspark.sql.functions.array_distinct</code> <code>pyspark.sql.functions.array_except</code> <code>pyspark.sql.functions.array_insert</code> <code>pyspark.sql.functions.array_intersect</code> <code>pyspark.sql.functions.array_join</code> <code>pyspark.sql.functions.array_max</code> <code>pyspark.sql.functions.array_min</code> <code>pyspark.sql.functions.array_position</code> <code>pyspark.sql.functions.array_prepend</code> <code>pyspark.sql.functions.array_remove</code> <code>pyspark.sql.functions.array_repeat</code> <code>pyspark.sql.functions.array_size</code> <code>pyspark.sql.functions.array_sort</code> <code>pyspark.sql.functions.array_union</code> <code>pyspark.sql.functions.arrays_overlap</code> <code>pyspark.sql.functions.arrays_zip</code> <code>pyspark.sql.functions.asc</code> <code>pyspark.sql.functions.asc_nulls_first</code> <code>pyspark.sql.functions.asc_nulls_last</code> <code>pyspark.sql.functions.ascii</code> <code>pyspark.sql.functions.asin</code> <code>pyspark.sql.functions.asinh</code> <code>pyspark.sql.functions.assert_true</code> <code>pyspark.sql.functions.atan</code> <code>pyspark.sql.functions.atan2</code> <code>pyspark.sql.functions.atanh</code> <code>pyspark.sql.functions.avg</code> <code>pyspark.sql.functions.base64</code> <code>pyspark.sql.functions.bin</code> <code>pyspark.sql.functions.bit_and</code> <code>pyspark.sql.functions.bit_count</code> <code>pyspark.sql.functions.bit_get</code> <code>pyspark.sql.functions.bit_length</code> <code>pyspark.sql.functions.bit_or</code> <code>pyspark.sql.functions.bit_xor</code> <code>pyspark.sql.functions.bitmap_bit_position</code> <code>pyspark.sql.functions.bitmap_bucket_number</code> <code>pyspark.sql.functions.bitmap_construct_agg</code> <code>pyspark.sql.functions.bitmap_count</code> <code>pyspark.sql.functions.bitmap_or_agg</code> <code>pyspark.sql.functions.bitwiseNOT</code> <code>pyspark.sql.functions.bitwise_not</code> <code>pyspark.sql.functions.bool_and</code> <code>pyspark.sql.functions.bool_or</code> <code>pyspark.sql.functions.broadcast</code> <code>pyspark.sql.functions.bround</code> <code>pyspark.sql.functions.btrim</code> <code>pyspark.sql.functions.bucket</code> <code>pyspark.sql.functions.call_function</code> <code>pyspark.sql.functions.call_udf</code> <code>pyspark.sql.functions.cardinality</code> <code>pyspark.sql.functions.cbrt</code> <code>pyspark.sql.functions.ceil</code> <code>pyspark.sql.functions.ceiling</code> <code>pyspark.sql.functions.char</code> <code>pyspark.sql.functions.char_length</code> <code>pyspark.sql.functions.character_length</code> <code>pyspark.sql.functions.coalesce</code> <code>pyspark.sql.functions.col</code> <code>pyspark.sql.functions.collect_list</code> <code>pyspark.sql.functions.collect_set</code> <code>pyspark.sql.functions.column</code> <code>pyspark.sql.functions.concat</code> <code>pyspark.sql.functions.concat_ws</code> <code>pyspark.sql.functions.contains</code> <code>pyspark.sql.functions.conv</code> <code>pyspark.sql.functions.convert_timezone</code> <code>pyspark.sql.functions.corr</code> <code>pyspark.sql.functions.cos</code> <code>pyspark.sql.functions.cosh</code> <code>pyspark.sql.functions.cot</code> <code>pyspark.sql.functions.count</code> <code>pyspark.sql.functions.countDistinct</code> <code>pyspark.sql.functions.count_distinct</code> <code>pyspark.sql.functions.count_if</code> <code>pyspark.sql.functions.count_min_sketch</code> <code>pyspark.sql.functions.covar_pop</code> <code>pyspark.sql.functions.covar_samp</code> <code>pyspark.sql.functions.crc32</code> <code>pyspark.sql.functions.create_map</code> <code>pyspark.sql.functions.csc</code> <code>pyspark.sql.functions.cume_dist</code> <code>pyspark.sql.functions.curdate</code> <code>pyspark.sql.functions.current_catalog</code> <code>pyspark.sql.functions.current_database</code> <code>pyspark.sql.functions.current_date</code> <code>pyspark.sql.functions.current_schema</code> <code>pyspark.sql.functions.current_timestamp</code> <code>pyspark.sql.functions.current_timezone</code> <code>pyspark.sql.functions.current_user</code> <code>pyspark.sql.functions.date_add</code> <code>pyspark.sql.functions.date_diff</code> <code>pyspark.sql.functions.date_format</code> <code>pyspark.sql.functions.date_from_unix_date</code> <code>pyspark.sql.functions.date_part</code> <code>pyspark.sql.functions.date_sub</code> <code>pyspark.sql.functions.date_trunc</code> <code>pyspark.sql.functions.dateadd</code> <code>pyspark.sql.functions.datediff</code> <code>pyspark.sql.functions.datepart</code> <code>pyspark.sql.functions.day</code> <code>pyspark.sql.functions.dayofmonth</code> <code>pyspark.sql.functions.dayofweek</code> <code>pyspark.sql.functions.dayofyear</code> <code>pyspark.sql.functions.days</code> <code>pyspark.sql.functions.decode</code> <code>pyspark.sql.functions.degrees</code> <code>pyspark.sql.functions.dense_rank</code> <code>pyspark.sql.functions.desc</code> <code>pyspark.sql.functions.desc_nulls_first</code> <code>pyspark.sql.functions.desc_nulls_last</code> <code>pyspark.sql.functions.e</code> <code>pyspark.sql.functions.element_at</code> <code>pyspark.sql.functions.elt</code> <code>pyspark.sql.functions.encode</code> <code>pyspark.sql.functions.endswith</code> <code>pyspark.sql.functions.equal_null</code> <code>pyspark.sql.functions.every</code> <code>pyspark.sql.functions.exists</code> <code>pyspark.sql.functions.exp</code> <code>pyspark.sql.functions.explode</code> <code>pyspark.sql.functions.explode_outer</code> <code>pyspark.sql.functions.expm1</code> <code>pyspark.sql.functions.expr</code> <code>pyspark.sql.functions.extract</code> <code>pyspark.sql.functions.factorial</code> <code>pyspark.sql.functions.filter</code> <code>pyspark.sql.functions.find_in_set</code> find_in_set only supports strings as the first argument, not dynamically another column like in pyspark. <code>pyspark.sql.functions.first</code> <code>pyspark.sql.functions.first_value</code> <code>pyspark.sql.functions.flatten</code> <code>pyspark.sql.functions.floor</code> <code>pyspark.sql.functions.forall</code> <code>pyspark.sql.functions.format_number</code> <code>pyspark.sql.functions.format_string</code> <code>pyspark.sql.functions.from_csv</code> <code>pyspark.sql.functions.from_json</code> <code>pyspark.sql.functions.from_unixtime</code> <code>pyspark.sql.functions.from_utc_timestamp</code> <code>pyspark.sql.functions.get</code> <code>pyspark.sql.functions.get_json_object</code> <code>pyspark.sql.functions.getbit</code> <code>pyspark.sql.functions.greatest</code> <code>pyspark.sql.functions.grouping</code> <code>pyspark.sql.functions.grouping_id</code> <code>pyspark.sql.functions.hash</code> <code>pyspark.sql.functions.hex</code> <code>pyspark.sql.functions.histogram_numeric</code> <code>pyspark.sql.functions.hll_sketch_agg</code> <code>pyspark.sql.functions.hll_sketch_estimate</code> <code>pyspark.sql.functions.hll_union</code> <code>pyspark.sql.functions.hll_union_agg</code> <code>pyspark.sql.functions.hour</code> <code>pyspark.sql.functions.hours</code> <code>pyspark.sql.functions.hypot</code> <code>pyspark.sql.functions.ifnull</code> <code>pyspark.sql.functions.ilike</code> <code>pyspark.sql.functions.initcap</code> <code>pyspark.sql.functions.inline</code> <code>pyspark.sql.functions.inline_outer</code> <code>pyspark.sql.functions.input_file_block_length</code> <code>pyspark.sql.functions.input_file_block_start</code> <code>pyspark.sql.functions.input_file_name</code> <code>pyspark.sql.functions.instr</code> <code>pyspark.sql.functions.isnan</code> <code>pyspark.sql.functions.isnotnull</code> <code>pyspark.sql.functions.isnull</code> <code>pyspark.sql.functions.java_method</code> <code>pyspark.sql.functions.json_array_length</code> <code>pyspark.sql.functions.json_object_keys</code> <code>pyspark.sql.functions.json_tuple</code> <code>pyspark.sql.functions.kurtosis</code> <code>pyspark.sql.functions.lag</code> <code>pyspark.sql.functions.last</code> <code>pyspark.sql.functions.last_day</code> <code>pyspark.sql.functions.last_value</code> <code>pyspark.sql.functions.lcase</code> <code>pyspark.sql.functions.lead</code> <code>pyspark.sql.functions.least</code> <code>pyspark.sql.functions.left</code> <code>pyspark.sql.functions.length</code> <code>pyspark.sql.functions.levenshtein</code> <code>pyspark.sql.functions.like</code> <code>pyspark.sql.functions.lit</code> <code>pyspark.sql.functions.ln</code> <code>pyspark.sql.functions.localtimestamp</code> <code>pyspark.sql.functions.locate</code> <code>pyspark.sql.functions.log</code> <code>pyspark.sql.functions.log10</code> <code>pyspark.sql.functions.log1p</code> <code>pyspark.sql.functions.log2</code> <code>pyspark.sql.functions.lower</code> <code>pyspark.sql.functions.lpad</code> <code>pyspark.sql.functions.ltrim</code> <code>pyspark.sql.functions.make_date</code> <code>pyspark.sql.functions.make_dt_interval</code> <code>pyspark.sql.functions.make_interval</code> <code>pyspark.sql.functions.make_timestamp</code> <code>pyspark.sql.functions.make_timestamp_ltz</code> <code>pyspark.sql.functions.make_timestamp_ntz</code> <code>pyspark.sql.functions.make_ym_interval</code> <code>pyspark.sql.functions.map_concat</code> <code>pyspark.sql.functions.map_contains_key</code> <code>pyspark.sql.functions.map_entries</code> <code>pyspark.sql.functions.map_filter</code> <code>pyspark.sql.functions.map_from_arrays</code> <code>pyspark.sql.functions.map_from_entries</code> <code>pyspark.sql.functions.map_keys</code> <code>pyspark.sql.functions.map_values</code> <code>pyspark.sql.functions.map_zip_with</code> <code>pyspark.sql.functions.mask</code> <code>pyspark.sql.functions.max</code> <code>pyspark.sql.functions.max_by</code> <code>pyspark.sql.functions.md5</code> <code>pyspark.sql.functions.mean</code> <code>pyspark.sql.functions.median</code> <code>pyspark.sql.functions.min</code> <code>pyspark.sql.functions.min_by</code> <code>pyspark.sql.functions.minute</code> <code>pyspark.sql.functions.mode</code> <code>pyspark.sql.functions.monotonically_increasing_id</code> <code>pyspark.sql.functions.month</code> <code>pyspark.sql.functions.months</code> <code>pyspark.sql.functions.months_between</code> <code>pyspark.sql.functions.named_struct</code> <code>pyspark.sql.functions.nanvl</code> <code>pyspark.sql.functions.negate</code> <code>pyspark.sql.functions.negative</code> <code>pyspark.sql.functions.next_day</code> <code>pyspark.sql.functions.now</code> <code>pyspark.sql.functions.nth_value</code> <code>pyspark.sql.functions.ntile</code> <code>pyspark.sql.functions.nullif</code> <code>pyspark.sql.functions.nvl</code> <code>pyspark.sql.functions.nvl2</code> <code>pyspark.sql.functions.octet_length</code> <code>pyspark.sql.functions.overlay</code> <code>pyspark.sql.functions.parse_url</code> <code>pyspark.sql.functions.percent_rank</code> <code>pyspark.sql.functions.percentile</code> <code>pyspark.sql.functions.percentile_approx</code> <code>pyspark.sql.functions.pi</code> <code>pyspark.sql.functions.pmod</code> <code>pyspark.sql.functions.posexplode</code> <code>pyspark.sql.functions.posexplode_outer</code> <code>pyspark.sql.functions.position</code> <code>pyspark.sql.functions.positive</code> <code>pyspark.sql.functions.pow</code> <code>pyspark.sql.functions.power</code> <code>pyspark.sql.functions.printf</code> <code>pyspark.sql.functions.product</code> <code>pyspark.sql.functions.quarter</code> <code>pyspark.sql.functions.radians</code> <code>pyspark.sql.functions.raise_error</code> <code>pyspark.sql.functions.rand</code> The seed value is accepted for API compatibility, but is unused. Even if set, the function will not be reproducible. <code>pyspark.sql.functions.randn</code> The seed value is accepted for API compatibility, but is unused. Even if set, the function will not be reproducible. <code>pyspark.sql.functions.rank</code> <code>pyspark.sql.functions.reduce</code> <code>pyspark.sql.functions.reflect</code> <code>pyspark.sql.functions.regexp</code> <code>pyspark.sql.functions.regexp_count</code> <code>pyspark.sql.functions.regexp_extract</code> <code>pyspark.sql.functions.regexp_extract_all</code> <code>pyspark.sql.functions.regexp_instr</code> <code>pyspark.sql.functions.regexp_like</code> <code>pyspark.sql.functions.regexp_replace</code> <code>pyspark.sql.functions.regexp_substr</code> <code>pyspark.sql.functions.regr_avgx</code> <code>pyspark.sql.functions.regr_avgy</code> <code>pyspark.sql.functions.regr_count</code> <code>pyspark.sql.functions.regr_intercept</code> <code>pyspark.sql.functions.regr_r2</code> <code>pyspark.sql.functions.regr_slope</code> <code>pyspark.sql.functions.regr_sxx</code> <code>pyspark.sql.functions.regr_sxy</code> <code>pyspark.sql.functions.regr_syy</code> <code>pyspark.sql.functions.repeat</code> <code>pyspark.sql.functions.replace</code> <code>pyspark.sql.functions.reverse</code> <code>pyspark.sql.functions.right</code> <code>pyspark.sql.functions.rint</code> <code>pyspark.sql.functions.rlike</code> <code>pyspark.sql.functions.round</code> <code>pyspark.sql.functions.row_number</code> <code>pyspark.sql.functions.rpad</code> <code>pyspark.sql.functions.rtrim</code> <code>pyspark.sql.functions.schema_of_csv</code> <code>pyspark.sql.functions.schema_of_json</code> <code>pyspark.sql.functions.sec</code> <code>pyspark.sql.functions.second</code> <code>pyspark.sql.functions.sentences</code> <code>pyspark.sql.functions.sequence</code> <code>pyspark.sql.functions.session_window</code> <code>pyspark.sql.functions.sha</code> <code>pyspark.sql.functions.sha1</code> <code>pyspark.sql.functions.sha2</code> <code>pyspark.sql.functions.shiftLeft</code> <code>pyspark.sql.functions.shiftRight</code> <code>pyspark.sql.functions.shiftRightUnsigned</code> <code>pyspark.sql.functions.shiftleft</code> <code>pyspark.sql.functions.shiftright</code> <code>pyspark.sql.functions.shiftrightunsigned</code> <code>pyspark.sql.functions.shuffle</code> <code>pyspark.sql.functions.sign</code> <code>pyspark.sql.functions.signum</code> <code>pyspark.sql.functions.sin</code> <code>pyspark.sql.functions.sinh</code> <code>pyspark.sql.functions.size</code> <code>pyspark.sql.functions.skewness</code> <code>pyspark.sql.functions.slice</code> <code>pyspark.sql.functions.some</code> <code>pyspark.sql.functions.sort_array</code> <code>pyspark.sql.functions.soundex</code> <code>pyspark.sql.functions.spark_partition_id</code> <code>pyspark.sql.functions.split</code> <code>pyspark.sql.functions.split_part</code> <code>pyspark.sql.functions.sqrt</code> <code>pyspark.sql.functions.stack</code> <code>pyspark.sql.functions.startswith</code> <code>pyspark.sql.functions.std</code> <code>pyspark.sql.functions.stddev</code> <code>pyspark.sql.functions.stddev_pop</code> <code>pyspark.sql.functions.stddev_samp</code> <code>pyspark.sql.functions.str_to_map</code> <code>pyspark.sql.functions.struct</code> <code>pyspark.sql.functions.substr</code> <code>pyspark.sql.functions.substring</code> <code>pyspark.sql.functions.substring_index</code> <code>pyspark.sql.functions.sum</code> <code>pyspark.sql.functions.sumDistinct</code> <code>pyspark.sql.functions.sum_distinct</code> <code>pyspark.sql.functions.tan</code> <code>pyspark.sql.functions.tanh</code> <code>pyspark.sql.functions.timestamp_micros</code> <code>pyspark.sql.functions.timestamp_millis</code> <code>pyspark.sql.functions.timestamp_seconds</code> <code>pyspark.sql.functions.toDegrees</code> <code>pyspark.sql.functions.toRadians</code> <code>pyspark.sql.functions.to_binary</code> <code>pyspark.sql.functions.to_char</code> <code>pyspark.sql.functions.to_csv</code> <code>pyspark.sql.functions.to_date</code> <code>pyspark.sql.functions.to_json</code> <code>pyspark.sql.functions.to_number</code> <code>pyspark.sql.functions.to_timestamp</code> <code>pyspark.sql.functions.to_timestamp_ltz</code> <code>pyspark.sql.functions.to_timestamp_ntz</code> <code>pyspark.sql.functions.to_unix_timestamp</code> <code>pyspark.sql.functions.to_utc_timestamp</code> <code>pyspark.sql.functions.to_varchar</code> <code>pyspark.sql.functions.transform</code> <code>pyspark.sql.functions.transform_keys</code> <code>pyspark.sql.functions.transform_values</code> <code>pyspark.sql.functions.translate</code> <code>pyspark.sql.functions.trim</code> <code>pyspark.sql.functions.trunc</code> <code>pyspark.sql.functions.try_add</code> <code>pyspark.sql.functions.try_aes_decrypt</code> <code>pyspark.sql.functions.try_avg</code> <code>pyspark.sql.functions.try_divide</code> <code>pyspark.sql.functions.try_element_at</code> <code>pyspark.sql.functions.try_multiply</code> <code>pyspark.sql.functions.try_subtract</code> <code>pyspark.sql.functions.try_sum</code> <code>pyspark.sql.functions.try_to_binary</code> <code>pyspark.sql.functions.try_to_number</code> <code>pyspark.sql.functions.try_to_timestamp</code> <code>pyspark.sql.functions.typeof</code> <code>pyspark.sql.functions.ucase</code> <code>pyspark.sql.functions.udf</code> <code>pyspark.sql.functions.udtf</code> <code>pyspark.sql.functions.unbase64</code> <code>pyspark.sql.functions.unhex</code> <code>pyspark.sql.functions.unix_date</code> <code>pyspark.sql.functions.unix_micros</code> <code>pyspark.sql.functions.unix_millis</code> <code>pyspark.sql.functions.unix_seconds</code> <code>pyspark.sql.functions.unix_timestamp</code> <code>pyspark.sql.functions.unwrap_udt</code> <code>pyspark.sql.functions.upper</code> <code>pyspark.sql.functions.url_decode</code> <code>pyspark.sql.functions.url_encode</code> <code>pyspark.sql.functions.user</code> <code>pyspark.sql.functions.var_pop</code> <code>pyspark.sql.functions.var_samp</code> <code>pyspark.sql.functions.variance</code> <code>pyspark.sql.functions.version</code> <code>pyspark.sql.functions.weekday</code> <code>pyspark.sql.functions.weekofyear</code> <code>pyspark.sql.functions.when</code> <code>pyspark.sql.functions.width_bucket</code> <code>pyspark.sql.functions.window</code> <code>pyspark.sql.functions.window_time</code> <code>pyspark.sql.functions.xpath</code> <code>pyspark.sql.functions.xpath_boolean</code> <code>pyspark.sql.functions.xpath_double</code> <code>pyspark.sql.functions.xpath_float</code> <code>pyspark.sql.functions.xpath_int</code> <code>pyspark.sql.functions.xpath_long</code> <code>pyspark.sql.functions.xpath_number</code> <code>pyspark.sql.functions.xpath_short</code> <code>pyspark.sql.functions.xpath_string</code> <code>pyspark.sql.functions.xxhash64</code> <code>pyspark.sql.functions.year</code> <code>pyspark.sql.functions.years</code> <code>pyspark.sql.functions.zip_with</code>"},{"location":"api_coverage/#datatypes-2329-79","title":"DataTypes (23/29 = 79%)","text":"API Implemented Notes <code>pyspark.sql.types.AnsiIntervalType</code> <code>pyspark.sql.types.ArrayType</code> <code>pyspark.sql.types.AtomicType</code> <code>pyspark.sql.types.BinaryType</code> <code>pyspark.sql.types.BooleanType</code> <code>pyspark.sql.types.ByteType</code> <code>pyspark.sql.types.CharType</code> <code>pyspark.sql.types.DataType</code> <code>pyspark.sql.types.DateType</code> <code>pyspark.sql.types.DayTimeIntervalType</code> <code>pyspark.sql.types.DecimalType</code> <code>pyspark.sql.types.DoubleType</code> <code>pyspark.sql.types.FloatType</code> <code>pyspark.sql.types.FractionalType</code> <code>pyspark.sql.types.IntegerType</code> <code>pyspark.sql.types.IntegralType</code> <code>pyspark.sql.types.LongType</code> <code>pyspark.sql.types.MapType</code> <code>pyspark.sql.types.NullType</code> <code>pyspark.sql.types.NumericType</code> <code>pyspark.sql.types.ShortType</code> <code>pyspark.sql.types.StringType</code> <code>pyspark.sql.types.StructField</code> <code>pyspark.sql.types.StructType</code> <code>pyspark.sql.types.TimestampNTZType</code> <code>pyspark.sql.types.TimestampType</code> <code>pyspark.sql.types.UserDefinedType</code> <code>pyspark.sql.types.VarcharType</code> <code>pyspark.sql.types.YearMonthIntervalType</code>"},{"location":"API%20Reference/","title":"Overview","text":"<p><code>pyspark-dubber</code> goal is bug-for-bug compatibility with <code>pyspark</code>, therefore it exposes the exact same API, and the pyspark API reference is a good starting point.</p> <p>The goal of the <code>pyspark-dubber</code> API reference is to document the know differences with the pyspark, which are usually due to incomplete implementation or cases where it is impossible to achieve the same behavior as in pyspark.</p> <p>If there is an undocumented deviation from the pyspark API, be it interface or behavior, it should be considered a bug (at a minimum a documentation bug) and reported here.</p>"},{"location":"API%20Reference/DataFrame/DataFrame.crossJoin/","title":"DataFrame.crossJoin","text":"<pre><code>DataFrame.crossJoin(\n    other: 'DataFrame',\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>pyspark allows duplicate column names, and by default does not prefix/suffix the columns of the other dataframe at all. Our backend (ibis) currently does not support duplicate column names, so this function suffixes all columns on other with '_right'.</p>"},{"location":"API%20Reference/DataFrame/DataFrame.drop/","title":"DataFrame.drop","text":"<pre><code>DataFrame.drop(\n    *cols: pyspark_dubber.sql.expr.Expr | str,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>Our backend (ibis) does not support duplicate column names like pyspark, therefore this function does not support dropping columns with the same name. You cannot anyway currently create such dataframes using <code>pyspark-dubber</code>.</p>"},{"location":"API%20Reference/DataFrame/DataFrame.filter/","title":"DataFrame.filter","text":"<pre><code>DataFrame.filter(\n    condition: pyspark_dubber.sql.expr.Expr | str,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>Using a string as a SQL expressions is not supported yet.</p>"},{"location":"API%20Reference/DataFrame/DataFrame.groupBy/","title":"DataFrame.groupBy","text":"<pre><code>DataFrame.groupBy(\n    *cols: pyspark_dubber.sql.expr.Expr | str,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>Currently only column names are supported for grouping, column expressions are not supported.</p>"},{"location":"API%20Reference/DataFrame/DataFrame.groupby/","title":"DataFrame.groupby","text":"<pre><code>DataFrame.groupby(\n    *cols: pyspark_dubber.sql.expr.Expr | str,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>Currently only column names are supported for grouping, column expressions are not supported.</p>"},{"location":"API%20Reference/DataFrame/DataFrame.orderBy/","title":"DataFrame.orderBy","text":"<pre><code>DataFrame.orderBy(\n    *cols: pyspark_dubber.sql.expr.Expr | str,\n    ascending: bool | list[bool] = True,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>Sorting by column ordinals (which are 1-based, not 0-based) is not supported yet. Additionally, this function still needs better testing around edge cases, when sorting with complex column expressions which include sorting.</p>"},{"location":"API%20Reference/DataFrame/DataFrame.show/","title":"DataFrame.show","text":"<pre><code>DataFrame.show(\n    n: int = 20,\n    truncate: bool | int = True,\n    vertical: bool = False,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>The <code>truncate</code> and <code>vertical</code> parameters are not honored. Additionally, the output is not printed justified exactly as pyspark as of the current version.</p>"},{"location":"API%20Reference/DataFrame/DataFrame.show/#example-pyspark-output","title":"Example pyspark output","text":"<pre><code>+----------+---+\n|First Name|Age|\n+----------+---+\n|     Scott| 50|\n|      Jeff| 45|\n|    Thomas| 54|\n|       Ann| 34|\n+----------+---+\n</code></pre>"},{"location":"API%20Reference/DataFrame/DataFrame.show/#example-pyspark-dubber-output","title":"Example pyspark-dubber output","text":"<pre><code>+----------+---+\n|First Name|Age|\n+----------+---+\n|Scott     |50 |\n|Jeff      |45 |\n|Thomas    |54 |\n|Ann       |34 |\n+----------+---+\n</code></pre>"},{"location":"API%20Reference/DataFrame/DataFrame.sort/","title":"DataFrame.sort","text":"<pre><code>DataFrame.sort(\n    *cols: pyspark_dubber.sql.expr.Expr | str,\n    ascending: bool | list[bool] = True,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>Sorting by column ordinals (which are 1-based, not 0-based) is not supported yet. Additionally, this function still needs better testing around edge cases, when sorting with complex column expressions which include sorting.</p>"},{"location":"API%20Reference/DataFrame/DataFrame.where/","title":"DataFrame.where","text":"<pre><code>DataFrame.where(\n    condition: pyspark_dubber.sql.expr.Expr | str,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>Using a string as a SQL expressions is not supported yet.</p>"},{"location":"API%20Reference/DataFrameReader/DataFrameReader.json/","title":"DataFrameReader.json","text":"<pre><code>DataFrameReader.json(\n    path: str | pathlib.Path | list[str | pathlib.Path],\n    schema: str | pyspark_dubber.sql.types.StructType | None = None,\n    *args,\n    **kwargs,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>Most parameters are accepted but completely ignored.</p> <p>The path cannot be a RDD like in pyspark for now. Schema is only supported as a StructType, DLL strings are unsupported.</p>"},{"location":"API%20Reference/pyspark_dubber.sql.functions/pyspark_dubber.sql.functions.find_in_set/","title":"pyspark_dubber.sql.functions.find_in_set","text":"<pre><code>pyspark_dubber.sql.functions.find_in_set(\n    str_: str,\n    str_array: pyspark_dubber.sql.expr.Expr | str,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>find_in_set only supports strings as the first argument, not dynamically another column like in pyspark.</p>"},{"location":"API%20Reference/pyspark_dubber.sql.functions/pyspark_dubber.sql.functions.rand/","title":"pyspark_dubber.sql.functions.rand","text":"<pre><code>pyspark_dubber.sql.functions.rand(\n    seed: int | None = None,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>The seed value is accepted for API compatibility, but is unused. Even if set, the function will not be reproducible.</p>"},{"location":"API%20Reference/pyspark_dubber.sql.functions/pyspark_dubber.sql.functions.randn/","title":"pyspark_dubber.sql.functions.randn","text":"<pre><code>pyspark_dubber.sql.functions.randn(\n    seed: int | None = None,\n)\n</code></pre> <p>PySpark API Reference</p> <p>Incompatibility Note</p> <p>The seed value is accepted for API compatibility, but is unused. Even if set, the function will not be reproducible.</p>"}]}